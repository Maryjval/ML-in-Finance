{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0109ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('MLF_GP1_CreditScore.csv',encoding = \"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b366b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46543fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sales/Revenues              0\n",
       "Gross Margin                0\n",
       "EBITDA                      0\n",
       "EBITDA Margin               0\n",
       "Net Income Before Extras    0\n",
       "Total Debt                  0\n",
       "Net Debt                    0\n",
       "LT Debt                     0\n",
       "ST Debt                     0\n",
       "Cash                        0\n",
       "Free Cash Flow              0\n",
       "Total Debt/EBITDA           0\n",
       "Net Debt/EBITDA             0\n",
       "Total MV                    0\n",
       "Total Debt/MV               0\n",
       "Net Debt/MV                 0\n",
       "CFO/Debt                    0\n",
       "CFO                         0\n",
       "Interest Coverage           0\n",
       "Total Liquidity             0\n",
       "Current Liquidity           0\n",
       "Current Liabilities         0\n",
       "EPS Before Extras           0\n",
       "PE                          0\n",
       "ROA                         0\n",
       "ROE                         0\n",
       "InvGrd                      0\n",
       "Rating                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fc4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X = df.iloc[:, :-2]  # All columns except the last two\n",
    "y_InvGrd = df.iloc[:, -2]  # second-to-last column\n",
    "y_Rating = df.iloc[:, -1]  # Last column\n",
    "\n",
    "X_train, X_test, y_InvGrd_train, y_InvGrd_test, y_Rating_train, y_Rating_test = train_test_split(\n",
    "    X, y_InvGrd, y_Rating, test_size=0.2, random_state=42) # 80% training and 20% test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473755e5",
   "metadata": {},
   "source": [
    "### Linear Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5397c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear regression with Ridge regularization :  0.7676470588235295\n"
     ]
    }
   ],
   "source": [
    "##Ridge Regularisation \n",
    "\n",
    "# Train the model\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_InvGrd_train)\n",
    "\n",
    "# Test the model\n",
    "y_InvGrd_pred = ridge.predict(X_test)\n",
    "y_InvGrd_pred[y_InvGrd_pred <= 0.5] = 0  # Set threshold to 0.5\n",
    "y_InvGrd_pred[y_InvGrd_pred > 0.5] = 1\n",
    "accuracy = accuracy_score(y_InvGrd_test, y_InvGrd_pred)\n",
    "\n",
    "print(\"Accuracy of Linear regression with Ridge regularization : \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60477d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear regression with Lasso regularization: 0.7529411764705882\n"
     ]
    }
   ],
   "source": [
    "##Lasso Regularisation\n",
    "\n",
    "# Train the model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_InvGrd_train)\n",
    "\n",
    "# Test the model\n",
    "y_InvGrd_pred = lasso.predict(X_test)\n",
    "y_InvGrd_pred[y_InvGrd_pred <= 0.5] = 0  # Set threshold to 0.5\n",
    "y_InvGrd_pred[y_InvGrd_pred > 0.5] = 1\n",
    "accuracy = accuracy_score(y_InvGrd_test, y_InvGrd_pred)\n",
    "\n",
    "print(\"Accuracy of Linear regression with Lasso regularization:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69466c19",
   "metadata": {},
   "source": [
    "### Logistic Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e245921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression with Ridge regularization : 0.7647058823529411\n",
      "Accuracy of Logistic regression with Lasso regularization : 0.7617647058823529\n"
     ]
    }
   ],
   "source": [
    "##Ridge Regularisation \n",
    "\n",
    "lr_ridge = LogisticRegression(penalty='l2', solver='liblinear', C=0.1)\n",
    "lr_ridge.fit(X_train, y_InvGrd_train)\n",
    "y_InvGrd_pred_ridge = lr_ridge.predict(X_test)\n",
    "accuracy_ridge = accuracy_score(y_InvGrd_test, y_InvGrd_pred_ridge)\n",
    "print(\"Accuracy of Logistic regression with Ridge regularization :\", accuracy_ridge)\n",
    "\n",
    "\n",
    "## Lasso regularization\n",
    "lr_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)\n",
    "lr_lasso.fit(X_train, y_InvGrd_train)\n",
    "y_InvGrd_pred_lasso = lr_lasso.predict(X_test)\n",
    "accuracy_lasso = accuracy_score(y_InvGrd_test, y_InvGrd_pred_lasso)\n",
    "print(\"Accuracy of Logistic regression with Lasso regularization :\", accuracy_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcda5a8",
   "metadata": {},
   "source": [
    "### Neural Networks Approach\n",
    "\n",
    "For the neural network approach, i will be loading the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b69e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e849a7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales/Revenues</th>\n",
       "      <th>Gross Margin</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>EBITDA Margin</th>\n",
       "      <th>Net Income Before Extras</th>\n",
       "      <th>Total Debt</th>\n",
       "      <th>Net Debt</th>\n",
       "      <th>LT Debt</th>\n",
       "      <th>ST Debt</th>\n",
       "      <th>Cash</th>\n",
       "      <th>...</th>\n",
       "      <th>Interest Coverage</th>\n",
       "      <th>Total Liquidity</th>\n",
       "      <th>Current Liquidity</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>EPS Before Extras</th>\n",
       "      <th>PE</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROE</th>\n",
       "      <th>InvGrd</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005496</td>\n",
       "      <td>0.030763</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.146849</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>-0.019296</td>\n",
       "      <td>-0.042648</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>-0.133716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136748</td>\n",
       "      <td>0.392143</td>\n",
       "      <td>-0.184887</td>\n",
       "      <td>0.062781</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.100409</td>\n",
       "      <td>0.163266</td>\n",
       "      <td>0.102521</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.005496</td>\n",
       "      <td>0.030763</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>0.146849</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>-0.019296</td>\n",
       "      <td>-0.042648</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>-0.133716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214657</td>\n",
       "      <td>0.392143</td>\n",
       "      <td>-0.184887</td>\n",
       "      <td>0.062781</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>0.163266</td>\n",
       "      <td>0.102521</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007045</td>\n",
       "      <td>0.023159</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.096440</td>\n",
       "      <td>0.108590</td>\n",
       "      <td>0.039410</td>\n",
       "      <td>0.034268</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.250371</td>\n",
       "      <td>0.101315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205290</td>\n",
       "      <td>0.483257</td>\n",
       "      <td>-0.017877</td>\n",
       "      <td>0.121357</td>\n",
       "      <td>0.110656</td>\n",
       "      <td>-0.045142</td>\n",
       "      <td>0.105711</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009396</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.099046</td>\n",
       "      <td>0.146137</td>\n",
       "      <td>0.030071</td>\n",
       "      <td>0.036938</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>0.356994</td>\n",
       "      <td>-0.052606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.996955</td>\n",
       "      <td>-0.122017</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>-0.008231</td>\n",
       "      <td>0.162421</td>\n",
       "      <td>0.132295</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009009</td>\n",
       "      <td>0.027714</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.034445</td>\n",
       "      <td>-0.034132</td>\n",
       "      <td>0.461894</td>\n",
       "      <td>-0.090869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172906</td>\n",
       "      <td>1.711426</td>\n",
       "      <td>-0.161561</td>\n",
       "      <td>0.084319</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.156427</td>\n",
       "      <td>0.225144</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.010702</td>\n",
       "      <td>-0.127046</td>\n",
       "      <td>-0.030785</td>\n",
       "      <td>-4.349569</td>\n",
       "      <td>0.124830</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.364676</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190974</td>\n",
       "      <td>0.603271</td>\n",
       "      <td>-0.150779</td>\n",
       "      <td>0.144187</td>\n",
       "      <td>-4.408257</td>\n",
       "      <td>-1.339781</td>\n",
       "      <td>-4.271318</td>\n",
       "      <td>-5.168673</td>\n",
       "      <td>0</td>\n",
       "      <td>Caa1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>-0.116919</td>\n",
       "      <td>-0.009799</td>\n",
       "      <td>-0.155183</td>\n",
       "      <td>-0.043333</td>\n",
       "      <td>-2.937747</td>\n",
       "      <td>0.157873</td>\n",
       "      <td>0.066243</td>\n",
       "      <td>0.084104</td>\n",
       "      <td>0.309846</td>\n",
       "      <td>0.808505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246259</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.604043</td>\n",
       "      <td>0.127468</td>\n",
       "      <td>-2.977064</td>\n",
       "      <td>-1.695900</td>\n",
       "      <td>-2.868086</td>\n",
       "      <td>-3.429429</td>\n",
       "      <td>0</td>\n",
       "      <td>Caa1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>-0.099676</td>\n",
       "      <td>0.067595</td>\n",
       "      <td>-0.170022</td>\n",
       "      <td>-0.078134</td>\n",
       "      <td>-1.960264</td>\n",
       "      <td>0.023226</td>\n",
       "      <td>-0.201398</td>\n",
       "      <td>0.024856</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>1.352542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266848</td>\n",
       "      <td>-0.959809</td>\n",
       "      <td>1.227363</td>\n",
       "      <td>0.056198</td>\n",
       "      <td>-1.955285</td>\n",
       "      <td>-1.919739</td>\n",
       "      <td>-1.876336</td>\n",
       "      <td>-1.940995</td>\n",
       "      <td>0</td>\n",
       "      <td>Caa1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>-0.088853</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>-0.122645</td>\n",
       "      <td>-0.037088</td>\n",
       "      <td>-2.066509</td>\n",
       "      <td>0.067495</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>0.289219</td>\n",
       "      <td>-0.171802</td>\n",
       "      <td>0.671224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226685</td>\n",
       "      <td>-0.438389</td>\n",
       "      <td>0.865331</td>\n",
       "      <td>-0.104059</td>\n",
       "      <td>-2.059347</td>\n",
       "      <td>-1.462429</td>\n",
       "      <td>-2.027254</td>\n",
       "      <td>-2.059961</td>\n",
       "      <td>0</td>\n",
       "      <td>Caa1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>-0.084739</td>\n",
       "      <td>-0.021961</td>\n",
       "      <td>-0.159290</td>\n",
       "      <td>-0.081442</td>\n",
       "      <td>-1.953166</td>\n",
       "      <td>-0.043481</td>\n",
       "      <td>-0.285966</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>1.987960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219651</td>\n",
       "      <td>-0.977050</td>\n",
       "      <td>2.167547</td>\n",
       "      <td>-0.056695</td>\n",
       "      <td>-1.948856</td>\n",
       "      <td>-1.918602</td>\n",
       "      <td>-1.952314</td>\n",
       "      <td>-1.758980</td>\n",
       "      <td>0</td>\n",
       "      <td>Caa1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sales/Revenues  Gross Margin    EBITDA  EBITDA Margin   \n",
       "0          -0.005496      0.030763  0.018885       0.024515  \\\n",
       "1          -0.005496      0.030763  0.088716       0.094733   \n",
       "2          -0.007045      0.023159  0.088716       0.096440   \n",
       "3          -0.009396      0.028400  0.088716       0.099046   \n",
       "4          -0.009009      0.027714  0.088716       0.098611   \n",
       "...              ...           ...       ...            ...   \n",
       "1695       -0.099327     -0.010702 -0.127046      -0.030785   \n",
       "1696       -0.116919     -0.009799 -0.155183      -0.043333   \n",
       "1697       -0.099676      0.067595 -0.170022      -0.078134   \n",
       "1698       -0.088853      0.007820 -0.122645      -0.037088   \n",
       "1699       -0.084739     -0.021961 -0.159290      -0.081442   \n",
       "\n",
       "      Net Income Before Extras  Total Debt  Net Debt   LT Debt   ST Debt   \n",
       "0                     0.146849   -0.029710 -0.019296 -0.042648  0.049875  \\\n",
       "1                     0.146849   -0.029710 -0.019296 -0.042648  0.049875   \n",
       "2                     0.108590    0.039410  0.034268  0.009059  0.250371   \n",
       "3                     0.146137    0.030071  0.036938 -0.016964  0.356994   \n",
       "4                     0.123500    0.024224  0.034445 -0.034132  0.461894   \n",
       "...                        ...         ...       ...       ...       ...   \n",
       "1695                 -4.349569    0.124830  0.146400  0.008407  0.364676   \n",
       "1696                 -2.937747    0.157873  0.066243  0.084104  0.309846   \n",
       "1697                 -1.960264    0.023226 -0.201398  0.024856  0.020078   \n",
       "1698                 -2.066509    0.067495 -0.011377  0.289219 -0.171802   \n",
       "1699                 -1.953166   -0.043481 -0.285966  0.045902 -0.179549   \n",
       "\n",
       "          Cash  ...  Interest Coverage  Total Liquidity  Current Liquidity   \n",
       "0    -0.133716  ...           0.136748         0.392143          -0.184887  \\\n",
       "1    -0.133716  ...           0.214657         0.392143          -0.184887   \n",
       "2     0.101315  ...           0.205290         0.483257          -0.017877   \n",
       "3    -0.052606  ...           0.232991         0.996955          -0.122017   \n",
       "4    -0.090869  ...           0.172906         1.711426          -0.161561   \n",
       "...        ...  ...                ...              ...                ...   \n",
       "1695 -0.028333  ...          -0.190974         0.603271          -0.150779   \n",
       "1696  0.808505  ...          -0.246259         0.007110           0.604043   \n",
       "1697  1.352542  ...          -0.266848        -0.959809           1.227363   \n",
       "1698  0.671224  ...          -0.226685        -0.438389           0.865331   \n",
       "1699  1.987960  ...          -0.219651        -0.977050           2.167547   \n",
       "\n",
       "      Current Liabilities  EPS Before Extras        PE       ROA       ROE   \n",
       "0                0.062781           0.148305  0.100409  0.163266  0.102521  \\\n",
       "1                0.062781           0.148305 -0.089598  0.163266  0.102521   \n",
       "2                0.121357           0.110656 -0.045142  0.105711  0.103378   \n",
       "3                0.079051           0.151639 -0.008231  0.162421  0.132295   \n",
       "4                0.084319           0.130435  0.015528  0.156427  0.225144   \n",
       "...                   ...                ...       ...       ...       ...   \n",
       "1695             0.144187          -4.408257 -1.339781 -4.271318 -5.168673   \n",
       "1696             0.127468          -2.977064 -1.695900 -2.868086 -3.429429   \n",
       "1697             0.056198          -1.955285 -1.919739 -1.876336 -1.940995   \n",
       "1698            -0.104059          -2.059347 -1.462429 -2.027254 -2.059961   \n",
       "1699            -0.056695          -1.948856 -1.918602 -1.952314 -1.758980   \n",
       "\n",
       "      InvGrd  Rating  \n",
       "0          1      A1  \n",
       "1          1      A1  \n",
       "2          1      A1  \n",
       "3          1      A1  \n",
       "4          1      A1  \n",
       "...      ...     ...  \n",
       "1695       0    Caa1  \n",
       "1696       0    Caa1  \n",
       "1697       0    Caa1  \n",
       "1698       0    Caa1  \n",
       "1699       0    Caa1  \n",
       "\n",
       "[1700 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('MLF_GP1_CreditScore.csv',encoding = \"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0541ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the features and its target variables\n",
    "X = df.iloc[:, :-2].values\n",
    "y_rating = df.iloc[:, -1].values\n",
    "y_invgrd = df.iloc[:, -2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b2cc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train  and test set of 80% and 20% respectively\n",
    "X_train, X_test, y_rating_train, y_rating_test, y_invgrd_train, y_invgrd_test = train_test_split(X, y_rating, y_invgrd, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e634c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_rating_train = label_encoder.fit_transform(y_rating_train)\n",
    "y_rating_test = label_encoder.transform(y_rating_test)\n",
    "\n",
    "# One-hot encode rating target variable\n",
    "y_rating_train = to_categorical(y_rating_train)\n",
    "y_rating_test = to_categorical(y_rating_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560ec05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22dd8715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43/43 [==============================] - 3s 21ms/step - loss: 3.0522 - accuracy: 0.0757 - val_loss: 2.8048 - val_accuracy: 0.1294\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.8023 - accuracy: 0.1368 - val_loss: 2.7587 - val_accuracy: 0.1735\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.7373 - accuracy: 0.1441 - val_loss: 2.7236 - val_accuracy: 0.1765\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.6775 - accuracy: 0.1699 - val_loss: 2.6858 - val_accuracy: 0.1706\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.6383 - accuracy: 0.1801 - val_loss: 2.6568 - val_accuracy: 0.1706\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.6048 - accuracy: 0.1838 - val_loss: 2.6310 - val_accuracy: 0.1676\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.5730 - accuracy: 0.1757 - val_loss: 2.6039 - val_accuracy: 0.1647\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.5615 - accuracy: 0.1838 - val_loss: 2.5719 - val_accuracy: 0.1735\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.5083 - accuracy: 0.1728 - val_loss: 2.5481 - val_accuracy: 0.1735\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.5042 - accuracy: 0.1853 - val_loss: 2.5310 - val_accuracy: 0.1794\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4497 - accuracy: 0.1971 - val_loss: 2.5211 - val_accuracy: 0.1765\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4683 - accuracy: 0.1838 - val_loss: 2.5117 - val_accuracy: 0.1794\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4404 - accuracy: 0.1809 - val_loss: 2.4944 - val_accuracy: 0.1794\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4278 - accuracy: 0.1926 - val_loss: 2.4881 - val_accuracy: 0.1882\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4470 - accuracy: 0.1882 - val_loss: 2.4788 - val_accuracy: 0.1971\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4179 - accuracy: 0.1978 - val_loss: 2.4708 - val_accuracy: 0.1941\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3983 - accuracy: 0.1941 - val_loss: 2.4669 - val_accuracy: 0.1941\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4044 - accuracy: 0.1904 - val_loss: 2.4584 - val_accuracy: 0.1971\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3909 - accuracy: 0.2066 - val_loss: 2.4576 - val_accuracy: 0.1853\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.4110 - accuracy: 0.1838 - val_loss: 2.4520 - val_accuracy: 0.1882\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3974 - accuracy: 0.1934 - val_loss: 2.4482 - val_accuracy: 0.1882\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3750 - accuracy: 0.2037 - val_loss: 2.4436 - val_accuracy: 0.1794\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3690 - accuracy: 0.2037 - val_loss: 2.4343 - val_accuracy: 0.1882\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3620 - accuracy: 0.2103 - val_loss: 2.4214 - val_accuracy: 0.1912\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3562 - accuracy: 0.2037 - val_loss: 2.4139 - val_accuracy: 0.1853\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3371 - accuracy: 0.2191 - val_loss: 2.4140 - val_accuracy: 0.1882\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3653 - accuracy: 0.1971 - val_loss: 2.4161 - val_accuracy: 0.1912\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3316 - accuracy: 0.2103 - val_loss: 2.4084 - val_accuracy: 0.1941\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3252 - accuracy: 0.2081 - val_loss: 2.4029 - val_accuracy: 0.2000\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3189 - accuracy: 0.2191 - val_loss: 2.3987 - val_accuracy: 0.1971\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3523 - accuracy: 0.2059 - val_loss: 2.4040 - val_accuracy: 0.1971\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3154 - accuracy: 0.2044 - val_loss: 2.4040 - val_accuracy: 0.2000\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3091 - accuracy: 0.2169 - val_loss: 2.4011 - val_accuracy: 0.2029\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3174 - accuracy: 0.2140 - val_loss: 2.3920 - val_accuracy: 0.2029\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3033 - accuracy: 0.2044 - val_loss: 2.3885 - val_accuracy: 0.2059\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3132 - accuracy: 0.2051 - val_loss: 2.3845 - val_accuracy: 0.2029\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.3028 - accuracy: 0.2081 - val_loss: 2.3767 - val_accuracy: 0.2088\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2835 - accuracy: 0.2368 - val_loss: 2.3757 - val_accuracy: 0.2059\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2828 - accuracy: 0.2184 - val_loss: 2.3792 - val_accuracy: 0.1971\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 2.2809 - accuracy: 0.2191 - val_loss: 2.3777 - val_accuracy: 0.2000\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2930 - accuracy: 0.2096 - val_loss: 2.3735 - val_accuracy: 0.1971\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2808 - accuracy: 0.2154 - val_loss: 2.3756 - val_accuracy: 0.2000\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2835 - accuracy: 0.2221 - val_loss: 2.3740 - val_accuracy: 0.2000\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2669 - accuracy: 0.2162 - val_loss: 2.3717 - val_accuracy: 0.1971\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.2882 - accuracy: 0.2110 - val_loss: 2.3714 - val_accuracy: 0.1941\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2724 - accuracy: 0.2243 - val_loss: 2.3705 - val_accuracy: 0.2059\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2640 - accuracy: 0.2206 - val_loss: 2.3808 - val_accuracy: 0.2000\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2747 - accuracy: 0.2110 - val_loss: 2.3784 - val_accuracy: 0.2029\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2698 - accuracy: 0.2397 - val_loss: 2.3680 - val_accuracy: 0.2118\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.2464 - accuracy: 0.2309 - val_loss: 2.3658 - val_accuracy: 0.2235\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.3658 - accuracy: 0.2235\n",
      "Neural network Accuracy: 22.35%\n"
     ]
    }
   ],
   "source": [
    "# Compiling\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_rating_train, epochs=50, batch_size=32, validation_data=(X_test, y_rating_test))\n",
    "\n",
    "# Evaluation\n",
    "_, accuracy = model.evaluate(X_test, y_rating_test)\n",
    "print('Neural network Accuracy: %.2f%%' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283a8b0",
   "metadata": {},
   "source": [
    "Based on the results gotten,\n",
    "\n",
    "Accuracy of Linear regression with Ridge regularization :  0.7676470588235295 approximately 0.7676\n",
    "\n",
    "Accuracy of Linear regression with Lasso regularization: 0.7529411764705882 approximately 0.7529\n",
    "\n",
    "Accuracy of Logistic regression with Ridge regularization : 0.7647058823529411 approximately 0.7647\n",
    "\n",
    "Accuracy of Logistic regression with Lasso regularization : 0.7617647058823529 approximately 0.7618\n",
    "\n",
    "Neural network Accuracy: 0.2235 \n",
    "\n",
    "The following observations regarding the effectiveness and suitability of each approach for the given problem could be deduced :\n",
    "\n",
    "Linear regression with Ridge regularization and Logistic regression with Ridge regularization have similar accuracy scores of 0.7676 and 0.7647 respectively. These results suggest that both these approaches are equally effective in predicting whether the firm is in an investment grade or not.\n",
    "\n",
    "Linear regression with Lasso regularization and Logistic regression with Lasso regularization also have similar accuracy scores of 0.7529 and 0.7618 respectively. However, these scores are slightly lower than the accuracy scores of the Ridge regularization models. This could be because Lasso regularization tends to produce sparse models, which might not be well suited for this problem.\n",
    "\n",
    "The neural network approach has a significantly lower accuracy score of 0.2235(22.35%). This indicates that the neural network model might not be suitable for this problem. However, it is possible that the model could be enhanced with further fine-tuning and optimization.\n",
    "\n",
    "In summary, based on the given results, it can be concluded that the Ridge regularization models are the most effective and suitable approaches for predicting whether the firm is in an investment grade or not. However, further analysis and experimentation might be necessary to confirm these findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c9067",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
